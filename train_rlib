from ray.tune.registry import register_env
from gym_ursina import make_env
import argparse
from ray.rllib.models import ModelCatalog
from models import rlib_model,MyPPOTorchPolicy,MyPPO
from ray.rllib.evaluation.rollout_worker import RolloutWorker
from ray.rllib.evaluation.worker_set import WorkerSet
from ray.rllib.policy.sample_batch import SampleBatch
from gym_ursina import UrsinaGym
import ray
from ray.rllib.algorithms.ppo.ppo import PPO,PPOConfig

parser = argparse.ArgumentParser(description='PPO')
parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate')
parser.add_argument('--max_grad_norm', type=float, default=0.5, help='Learning rate for discriminator')
parser.add_argument('--critic_loss_coeff', type=float, default=0.5, help='Learning rate for discriminator')
parser.add_argument('--entropy_coeff', type=float, default=0.01, help='Learning rate for discriminator')
parser.add_argument('--bs', type=int, default=1000, help='Batch size')
parser.add_argument('--ppo_epochs', type=int, default=5, help='Number of epochs')
parser.add_argument('--text_input_length', type=int, default=34, help='406 Number of features in text input')
parser.add_argument('--depth_map_length', type=int, default=0, help='361 Number of features in text input')
parser.add_argument('--action_direction_length', type=int, default=29, help='possible actions')
parser.add_argument('--max_action_length', type=int, default=10, help='the max action length')
parser.add_argument('--num_steps', type=int, default=2000, help='number of steps per epoch')
parser.add_argument('--test_steps', type=int, default=10000, help='number of steps per epoch')
parser.add_argument('--seed', type=int, default=7, help='seed to initialize libraries')
parser.add_argument('--max_iter', type=int, default=3000000, help='max number of steps')
parser.add_argument('--load_model', type=bool, default=False, help='load a pretrained model')
parser.add_argument('--compute_dynamic_stat', type=bool, default=True, help='collect the agents data in parallel')
parser.add_argument('--anneal_lr', type=bool, default= False, help='collect the agents data in parallel')
parser.add_argument('--parallel_workers_test', type=int, default=1, help='number of parallel agents')
parser.add_argument('--parallel_workers', type=int, default=2, help='number of parallel agents')
parser.add_argument('--envs_per_worker', type=int, default=2, help='number of parallel agents')
parser.add_argument('--sageMaker', type=bool, default=False, help='number of parallel agents')




if __name__ == '__main__':
    args = parser.parse_args()



    register_env("UrsinaGym", make_env)
    ModelCatalog.register_custom_model("rlib_model", rlib_model)

    """ config = {
    "framework": "torch",
    "num_workers": 1,
    "num_gpus_per_worker": 1/args.parallel_workers,
    "num_cpus_per_worker":1,
    "num_envs_per_worker": 2,
    "custom_resources_per_worker":None,
    "remote_worker_envs": True,
    "remote_env_batch_wait_ms": 1,
    "rollout_fragment_length": 100,
    "batch_mode": "complete_episodes",
    "train_batch_size": 1000,
    "gamma": 0.99,
    "lr": 0.0001,
    "env":"UrsinaGym",
    "env_config":{
        "obs_size":34,
        "mask_size":29,"min_size":75,
        "max_size":250,
        "is_teacher":True},
    "model": {
         "custom_model":"rlib_model",
         "custom_model_config": {
            "obs":args.text_input_length,
            "action_mask": args.action_direction_length,
            "hidden_layer":128}},
    "recreate_failed_workers":True,
    "restart_failed_sub_environments":True,

    }


    model_config = {
        "name":"rlib_model",
        "obs":args.text_input_length,
        "action_mask": args.action_direction_length,
        "hidden_layer":128}
    
    env_config = {
        "obs_size":34,
        "mask_size":29,
        "min_size":75,
        "max_size":250,
        "is_teacher":False} 


    worker = RolloutWorker(env_creator=lambda _: make_env(env_config),rollout_fragment_length=100,
    episode_horizon=50
    ,policy_spec=MyPPOTorchPolicy,policy_config={"framework": "torch"},model_config=model_config,
     num_envs=6,remote_worker_envs=6,batch_mode="complete_episodes") 

    samples = worker.sample()
    print("final samples", samples)

    print(worker.num_envs)
    print(worker.num_workers)    """

    config = PPOConfig(algo_class=MyPPO)
    config.model = {
         "custom_model":"rlib_model",
         "custom_model_config": {
            "obs":args.text_input_length,
            "action_mask": args.action_direction_length,
            "hidden_layer":128}}  
    config.env = "UrsinaGym"
    config.env_config = {
            "obs_size":34,
            "mask_size":29,
            "min_size":75,
            "max_size":250,
            "is_teacher":False}
    config.framework(framework="torch")
    config.num_envs_per_worker = 1
    config.num_workers = 0
    #config.remote_worker_envs = True    
    config.num_gpus_per_worker = 1/args.parallel_workers
    config.train_batch_size = 1000
    config.rollout_fragment_length = 100
    config.batch_mode = "complete_episodes"

    algo = config.build()
    #config = config.to_dict()

    # Create our RLlib Trainer.
    #algo = MyPPO(config=config) 
    #config = PPOConfig().environment(env="CartPole-v0").training(train_batch_size=4000).framework(framework="torch")
    #algo = config.build()


    # Run it for n training iterations. A training iteration includes
    # parallel sample collection by the environment workers as well as
    # loss calculation on the collected batch and a model update.
    """ for _ in range(3):
        print(algo.train()) """
    """ config = PPOConfig(algo_class=MyPPO)
    config.horizon = 200
    config.num_envs_per_worker = 1
    config.num_workers = 1
    config.env_config = env_config
    config.model = model_config
    config.framework(framework="torch")
    config = config.build(env=UrsinaGym) """
        
    """ env = make_env(env_config)
    policy = MyPPOTorchPolicy(env.observation_space,env.action_space,{})
    workers = WorkerSet(
    policy_class=MyPPOTorchPolicy,
    env_creator=lambda c: make_env(env_config),
    num_workers=1,trainer_config=config) """

""" for i in range(3):
    # Gather a batch of samples.
    T1 = SampleBatch.concat_samples(
        ray.get([w.sample.remote() for w in workers.remote_workers()]))

    # Improve the policy using the T1 batch.
    policy.learn_on_batch(T1)

    # The local worker acts as a "parameter server" here.
    # We put the weights of its `policy` into the Ray object store once (`ray.put`)...
    weights = ray.put({"default_policy": policy.get_weights()})
    for w in workers.remote_workers():
        # ... so that we can broacast these weights to all rollout-workers once.
        w.set_weights.remote(weights)
 """