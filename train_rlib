from ray.tune.registry import register_env
from gym_ursina import make_env
import argparse
from ray.rllib.models import ModelCatalog
from models import rlib_model,MyPPOTorchPolicy,MyPPO
from ray.rllib.evaluation.rollout_worker import RolloutWorker
from ray.rllib.evaluation.worker_set import WorkerSet
from ray.rllib.policy.sample_batch import SampleBatch
from gym_ursina import UrsinaGym
import ray
from ray.rllib.algorithms.ppo.ppo import PPO,PPOConfig
from gym.spaces import Box,Discrete,Dict
from ray import tune
from call_backs import MyCallBacks

parser = argparse.ArgumentParser(description='PPO')
parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate')
parser.add_argument('--max_grad_norm', type=float, default=0.5, help='Learning rate for discriminator')
parser.add_argument('--critic_loss_coeff', type=float, default=0.5, help='Learning rate for discriminator')
parser.add_argument('--entropy_coeff', type=float, default=0.01, help='Learning rate for discriminator')
parser.add_argument('--bs', type=int, default=1000, help='Batch size')
parser.add_argument('--ppo_epochs', type=int, default=5, help='Number of epochs')
parser.add_argument('--text_input_length', type=int, default=34, help='406 Number of features in text input')
parser.add_argument('--depth_map_length', type=int, default=0, help='361 Number of features in text input')
parser.add_argument('--action_direction_length', type=int, default=29, help='possible actions')
parser.add_argument('--max_action_length', type=int, default=10, help='the max action length')
parser.add_argument('--num_steps', type=int, default=2000, help='number of steps per epoch')
parser.add_argument('--test_steps', type=int, default=10000, help='number of steps per epoch')
parser.add_argument('--seed', type=int, default=7, help='seed to initialize libraries')
parser.add_argument('--max_iter', type=int, default=3000000, help='max number of steps')
parser.add_argument('--load_model', type=bool, default=False, help='load a pretrained model')
parser.add_argument('--compute_dynamic_stat', type=bool, default=True, help='collect the agents data in parallel')
parser.add_argument('--anneal_lr', type=bool, default= False, help='collect the agents data in parallel')
parser.add_argument('--parallel_workers_test', type=int, default=1, help='number of parallel agents')
parser.add_argument('--parallel_workers', type=int, default=2, help='number of parallel agents')
parser.add_argument('--envs_per_worker', type=int, default=2, help='number of parallel agents')
parser.add_argument('--sageMaker', type=bool, default=False, help='number of parallel agents')




if __name__ == '__main__':
    args = parser.parse_args()



    register_env("UrsinaGym", lambda config: UrsinaGym(config))
    ModelCatalog.register_custom_model("rlib_model", rlib_model)

    """ model_config = {
        "name":"rlib_model",
        "obs":args.text_input_length}
    
    env_config = {
        "obs_size":34,
        "mask_size":29,
        "min_size":75,
        "max_size":250,
        "is_teacher":False} 


    worker = RolloutWorker(env_creator=lambda _: make_env(env_config),rollout_fragment_length=100,
    episode_horizon=50
    ,policy_spec=MyPPOTorchPolicy,policy_config={"framework": "torch","create_env_on_driver":True},model_config=model_config,
     num_envs=2,remote_worker_envs=True,num_workers=3,batch_mode="complete_episodes")  

    samples = worker.sample()
    print("final samples", samples)  """

    env_config = {
            "obs_size":34,
            "mask_size":29,
            "min_size":75,
            "max_size":250,
            "is_teacher":False}

    config = PPOConfig(algo_class=MyPPO)
    config.env = "UrsinaGym"
    config.env_config = env_config
    config.framework(framework="torch")
    config.num_envs_per_worker = 3
    config.num_workers = 4
    config.remote_worker_envs = True  
    config.num_gpus_per_worker = 1/config.num_workers 
    config.remote_env_batch_wait_ms = 4
    config.disable_env_checking = False
    config.recreate_failed_workers= True
    config.restart_failed_sub_environments = True
    config.train_batch_size = 4000
    config.lr = 5e-5
    config.model["vf_share_layers"] = False
    config.model["custom_model"] = "rlib_model"
    config.model["custom_model_config"] = {"obs":args.text_input_length}
    config.batch_mode = "complete_episodes"
    config.horizon = 100
    config.log_level = "INFO"
    config.simple_optimizer = True

    
    tune.register_trainable("MyPPO",MyPPO)

    tune.run("MyPPO",config=config.to_dict())  

    """ config = PPOConfig().environment(env="CartPole-v0").training(train_batch_size=4000).framework(framework="torch")

    tune.run("PPO", config=config.to_dict()) """
    #algo = config.build()
    #algo.train()

    #policy = algo.get_policy()
    #print(policy.model.model)
   
    """ config = config.to_dict()
    obs = Dict({
            "obs": Box(-100., 100., shape=(34,)),
            "action_mask": Box(0, 1, shape=(29,))
        })
    action = Discrete(29)
    
    policy = MyPPOTorchPolicy(obs,action,{"custom_model":"rlib_model",
    "custom_model_config":{"obs":args.text_input_length}})
    workers = WorkerSet(
    policy_class=MyPPOTorchPolicy,
    env_creator=lambda c: make_env(env_config),
    num_workers=1,trainer_config=config) 

    for i in range(1):
    # Gather a batch of samples.
        T1 = SampleBatch.concat_samples(
            ray.get([w.sample.remote() for w in workers.remote_workers()]))

        # Improve the policy using the T1 batch.
        policy.learn_on_batch(T1)

        # The local worker acts as a "parameter server" here.
        # We put the weights of its `policy` into the Ray object store once (`ray.put`)...
        weights = ray.put({"default_policy": policy.get_weights()})
        for w in workers.remote_workers():
            # ... so that we can broacast these weights to all rollout-workers once.
            w.set_weights.remote(weights)   """ 
 