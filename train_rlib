from ray.tune.registry import register_env
from gym_ursina import make_env
import argparse
from ray.rllib.models import ModelCatalog
from models import rlib_model,MyPPO
from ray.rllib.evaluation.rollout_worker import RolloutWorker
from ray.rllib.evaluation.worker_set import WorkerSet
from ray.rllib.policy.sample_batch import SampleBatch
from gym_ursina import UrsinaGym
import ray
from ray.rllib.algorithms.ppo.ppo import PPO,PPOConfig
from gym.spaces import Box,Discrete,Dict
from ray import tune
from ray.air.config import RunConfig
from ray.air import FailureConfig
from call_backs import MyCallBacks
from Teacher import get_args
from ray.tune.logger import pretty_print
from ray.tune.schedulers.pb2 import PB2
from ray.rllib.algorithms.registry import POLICIES

parser = argparse.ArgumentParser(description='PPO')
parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate')
parser.add_argument('--max_grad_norm', type=float, default=0.5, help='Learning rate for discriminator')
parser.add_argument('--critic_loss_coeff', type=float, default=0.5, help='Learning rate for discriminator')
parser.add_argument('--entropy_coeff', type=float, default=0.01, help='Learning rate for discriminator')
parser.add_argument('--bs', type=int, default=1000, help='Batch size')
parser.add_argument('--ppo_epochs', type=int, default=5, help='Number of epochs')
parser.add_argument('--text_input_length', type=int, default=34, help='406 Number of features in text input')
parser.add_argument('--depth_map_length', type=int, default=0, help='361 Number of features in text input')
parser.add_argument('--action_direction_length', type=int, default=29, help='possible actions')
parser.add_argument('--max_action_length', type=int, default=10, help='the max action length')
parser.add_argument('--num_steps', type=int, default=2000, help='number of steps per epoch')
parser.add_argument('--test_steps', type=int, default=10000, help='number of steps per epoch')
parser.add_argument('--seed', type=int, default=7, help='seed to initialize libraries')
parser.add_argument('--max_iter', type=int, default=3000000, help='max number of steps')
parser.add_argument('--load_model', type=bool, default=False, help='load a pretrained model')
parser.add_argument('--compute_dynamic_stat', type=bool, default=True, help='collect the agents data in parallel')
parser.add_argument('--anneal_lr', type=bool, default= False, help='collect the agents data in parallel')
parser.add_argument('--parallel_workers_test', type=int, default=1, help='number of parallel agents')
parser.add_argument('--parallel_workers', type=int, default=2, help='number of parallel agents')
parser.add_argument('--envs_per_worker', type=int, default=2, help='number of parallel agents')
parser.add_argument('--sageMaker', type=bool, default=False, help='number of parallel agents')



if __name__ == '__main__':
    args = parser.parse_args()



    register_env("UrsinaGym", lambda config: UrsinaGym(config))
    ModelCatalog.register_custom_model("rlib_model", rlib_model)
    tune.register_trainable("MyPPO",MyPPO)

    """ model_config = {
        "name":"rlib_model",
        "obs":args.text_input_length}
    
    env_config = {
        "obs_size":34,
        "mask_size":29,
        "min_size":75,
        "max_size":250,
        "is_teacher":False} 


    worker = RolloutWorker(env_creator=lambda _: make_env(env_config),rollout_fragment_length=100,
    episode_horizon=50
    ,policy_spec=MyPPOTorchPolicy,policy_config={"framework": "torch","create_env_on_driver":True},model_config=model_config,
     num_envs=2,remote_worker_envs=True,num_workers=3,batch_mode="complete_episodes")  

    samples = worker.sample()
    print("final samples", samples)  """

    teacher_args = get_args()

    env_config = {
            "obs_size":34,
            "mask_size":29,
            "min_size":75,
            "max_size":250,
            "is_teacher":False,
            "teacher_args":teacher_args}

    #config = PPOConfig(algo_class=PPO)
    config = PPOConfig(algo_class=MyPPO)
    config.env = "UrsinaGym"
    #config.env_config = env_config
    config.env_config.update(env_config)
    config.framework(framework="torch")
    config.num_envs_per_worker = 2
    config.num_workers = 1
    config.remote_worker_envs = True  
    config.num_gpus = 1/4           
    config.num_gpus_per_worker = 1/8
    config.num_cpus_per_worker = config.num_envs_per_worker
    config.remote_env_batch_wait_ms = 4
    config.disable_env_checking = False
    config.recreate_failed_workers= True
    config.restart_failed_sub_environments = True
    config.train_batch_size = 1100#tune.uniform(0,1)#tune.qrandint(1000, 10000, 500)
    config.lr = tune.loguniform(1e-5,1e-1)#tune.grid_search([0.01, 0.001, 0.0001,0.00001])
    config.sgd_minibatch_size = tune.choice([128, 256, 512,1024])
    config.num_sgd_iter = tune.randint(5,60)
    config.clip_param = 0.3#tune.grid_search([0.1,0.2,0.3])
    config.model["fcnet_hiddens"] = [256, 256]
    config.model["fcnet_activation"] = "tanh"
    config.model["vf_share_layers"] = False#tune.grid_search([True, False])
    config.model["custom_model"] = "rlib_model"
    config.model["custom_model_config"] = {"obs":args.text_input_length}
    config.batch_mode = "complete_episodes"#"complete_episodes"#"truncate_episodes"
    config.horizon = 100
    config.log_level = "WARN"#"INFO"
    config.simple_optimizer = True
    config.create_env_on_local_worker = False
    
    #resource_group = tune.PlacementGroupFactory([{"CPU": 2, "GPU": 0.25}])
    #trainable_with_resources = tune.with_resources(MyPPO, resource_group)

    pb2_scheduler = PB2(
        time_attr="training_iteration",
        metric='episode_reward_mean',
        mode='max',
        perturbation_interval=1,
        synch=True,
        hyperparam_bounds={"lr": [0.00001,0.001],
                            "sgd_minibatch_size":[128,1024],
                            "num_sgd_iter":[5,60]})

    """ tune.run("MyPPO",scheduler=pb2_scheduler,num_samples=4,
    fail_fast=True,verbose=3,reuse_actors=True,config=config.to_dict()) """
    
    tune.Tuner(
    "MyPPO",
    param_space=config.to_dict(),
    tune_config=tune.TuneConfig(num_samples=5,scheduler=pb2_scheduler,reuse_actors=True),
    run_config=RunConfig(failure_config=FailureConfig(fail_fast=True))
    ).fit()  

    #tune.run("MyPPO",config=config.to_dict())  

    """ config = PPOConfig().environment(env="CartPole-v0").training(train_batch_size=4000).framework(framework="torch")

    tune.run("PPO", config=config.to_dict()) """

    """ def ensaa(worker:RolloutWorker):
        print(worker.global_vars)
    algo = config.build()
    algo.workers.foreach_worker(ensaa) """




    #algo.train()

    #policy = algo.get_policy()
    #print(policy.model.model)
